{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model subclassing and custom training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Coding tutorials\n",
    " #### [1. Model subclassing](#coding_tutorial_1)\n",
    " #### [2. Custom layers](#coding_tutorial_2)\n",
    " #### [3. Automatic differentiation](#coding_tutorial_3)\n",
    " #### [4. Custom training loops](#coding_tutorial_4)\n",
    " #### [5. tf.function decorator](#coding_tutorial_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_1\"></a>\n",
    "## Model subclassing\n",
    "\n",
    "Es un metodo para definir modelos que da un nivel bajo de control en cuanto a la construccion y la operacion,\n",
    "se trata de sobreescribir la clase Model dada por keras para que nuestros modelos hereden sus metodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nota \n",
    "#*args sirve para que la funcion espere un numero variable de argumentos y no definido asi\n",
    "def f(x,y): #solo sirve para 2 variables\n",
    "    return x,y\n",
    "\n",
    "def f(*args): #funciona para un numero arbitrario de parametros\n",
    "    for elemento in args:\n",
    "        #has algo con los parametros\n",
    "        \n",
    "# * es el que indica este comportamiento ie podemos ocupar cualquier nombre como *parametros *param        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**kwargs tiene la misma funcion pero recibe los distintos parametros como un diccionario\n",
    "#** es lo que indica este comportamiento ie podemos ocupar cualquier nombre como **parametros **param\n",
    "\n",
    "def f(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        #has algo con los parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estructura basica de model subclassing\n",
    "\n",
    "#importamos clase modelo y capas que ocuparemos para el modelo\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense.Dropout\n",
    "\n",
    "#definimos una clase y le pasamos la clase Model para indicar que hereda su estrucura (subclasing)\n",
    "class MiModelo(Model):\n",
    "    \n",
    "    #Sobreescribimos el metodo init aqui creamos nuestras capas\n",
    "    def __init__(self,numero_clases,**kwargs):\n",
    "        super(MiModelo,self).__init__(**kwargs)\n",
    "        #aqui definimos las capas que tiene nuestro modelo\n",
    "        self.dense1=Dense(16, activation='sigmoid')\n",
    "        self.dropout=Droput(0.5)\n",
    "        self.dense2=Dense(numero_clases,activation='softmax')\n",
    "        \n",
    "    #sobreescribimos el metodo call que define el paso hacia adelante     \n",
    "    def call(self,inputs,Opcion=False):\n",
    "        #aplicamos las capas a las entradas ie aplica el modelo\n",
    "        h=self.dense1(inputs)\n",
    "        #con el booleano en training controlamos que droput se ejecute en entrenamiento pero no en test\n",
    "        #es decir sirve para reglar el comportamiento\n",
    "        h=self.dropout(h,training=Opcion)\n",
    "        return self.dense2(h)\n",
    "    \n",
    "#instanciando la clase\n",
    "mi_modelo=MiModelo(10,name='mi_modelo')\n",
    "\n",
    "#ahora podemos usar los metodos .fit() .train() como siempre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Softmax, concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a simple model using the model subclassing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "class MyModel(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.dense1=Dense(64, activation='relu')\n",
    "        self.dense2=Dense(10)\n",
    "        self.dropout=Dropout(0.4)\n",
    "        \n",
    "         \n",
    "    def call(self,inputs,training=True):\n",
    "        h=self.dense1(inputs)\n",
    "        if training:\n",
    "            h=self.dropout(h)\n",
    "        return self.dense2(h)\n",
    "\n",
    "#podemos generar modelos mas dinamicos\n",
    "class MyModel2(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyModel2,self).__init__()\n",
    "        self.dense1=Dense(64, activation='relu')\n",
    "        self.dense2=Dense(10)\n",
    "        self.dense3=Dense(5)\n",
    "        self.softmax=Softmax()\n",
    "        \n",
    "         \n",
    "    def call(self,inputs):\n",
    "        #entrada se pasan a dos capas distintas\n",
    "        x=self.dense1(inputs)\n",
    "        y1=self.dense2(inputs)\n",
    "        y2=self.dense3(y1)\n",
    "        concat=concatenate([x,y2])#note que esta no esta en el modelo la importamos y la usamos\n",
    "        return self.softmax(concat)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              multiple                  704       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  650       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "=================================================================\n",
      "Total params: 1,354\n",
      "Trainable params: 1,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "model=MyModel()\n",
    "model(tf.random.uniform([1,10]))#pasamos entrada random para que genere pesos pues no lo compilamos\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              multiple                  704       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  110       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  55        \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            multiple                  0         \n",
      "=================================================================\n",
      "Total params: 869\n",
      "Trainable params: 869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=MyModel2()\n",
    "model(tf.random.uniform([1,10]))#pasamos entrada random para que genere pesos pues no lo compilamos\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_2\"></a>\n",
    "## Custom layers\n",
    "\n",
    "Hasta ahora hemos usado las capas que tf.keras.layers ofrecen para nosotros pero tambien podemos definir nuestras propias capas con el comportamiento que deseemos, la forma en que podemos hacerlo es sobreescribiendo nuestra propia clase modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos tensorflow y la clase Layer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "#dotamos la clase con la herencia de Layer\n",
    "class MiCapa(Layer):\n",
    "    \n",
    "    #Creamos las variables de la capa en el inicializador\n",
    "    def __init__(self,input_dim,units):\n",
    "        super(MiCapa,self).__init__()\n",
    "        pesos_init=tf.random_normal_initializer() #definimos como inicializamos los pesos\n",
    "        # definimos los pesos como un tensor variable con el inicializador que definimos y la forma deseada\n",
    "        self.w=tf.Variable(initial_value=pesos_init(shape=(input_dim,units)))\n",
    "        \n",
    "     #Ojo no se necsitan dos init esta es una forma alterna de definir las variables\n",
    "    def __init__(self,input_dim,units):\n",
    "        super(MiCapa,self).__init__()\n",
    "        #es una forma reducida con los pesos inicializados solo pasamos forma e inicializador\n",
    "        #al metodo add_weight ambas formas son equivalentes\n",
    "        self.w=self.add_weight(shape=(input_dim,units),initializer='random_normal')    \n",
    "    \n",
    "    #definimos los calculos que hace la capa\n",
    "    def call(self,inputs):\n",
    "        return tf.matmul(inputs,self.w)\n",
    "    \n",
    "#instanciando capa\n",
    "mi_capa=MiCapa(3,2)\n",
    "#ejecutando capa en un tensor\n",
    "inputs=tf.ones((1,3))\n",
    "mi_capa(inputs)\n",
    "\n",
    "#podemos acceder a los metodos de capa como .weights \n",
    "#tambien ya podemos usa estas capas en nuestros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1. 1. 1. 1. 1.]], shape=(1, 5), dtype=float32)\n",
      "\n",
      "tf.Tensor([[-0.05633927 -0.22896083  0.10024416]], shape=(1, 3), dtype=float32)\n",
      "\n",
      "[<tf.Variable 'Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[-0.00614806, -0.06634471,  0.04945759],\n",
      "       [ 0.06338884, -0.02636651,  0.02478405],\n",
      "       [ 0.03693418, -0.03068536, -0.01494657],\n",
      "       [-0.0628328 , -0.11265617, -0.01375101],\n",
      "       [-0.08768144,  0.00709192,  0.05470009]], dtype=float32)>, <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# Create a custom layer\n",
    "class MyLayer(Layer):\n",
    "         \n",
    "    def __init__(self,units,input_dim):\n",
    "        super(MyLayer,self).__init__()\n",
    "        self.w=self.add_weight(shape=(input_dim,units),initializer='random_normal') #inicializa pesos   \n",
    "        self.b=self.add_weight(shape=(units,),initializer='zeros') #inicializa sesgos\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        return tf.matmul(inputs,self.w)+self.b\n",
    "    \n",
    "my_layer=MyLayer(3,5)\n",
    "x=tf.ones((1,5))\n",
    "print(x)\n",
    "print()\n",
    "print(my_layer(x))\n",
    "print()\n",
    "print(my_layer.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable weights: 2\n",
      "non-trainable weights: 0\n"
     ]
    }
   ],
   "source": [
    "#como no hemos specificados que pesos son entrenables tenemos que tanto pesos como sesgos son entrenables\n",
    "print('trainable weights:', len(my_layer.trainable_weights))\n",
    "print('non-trainable weights:', len(my_layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify trainable weights\n",
    "class MyLayer(Layer):\n",
    "         \n",
    "    def __init__(self,units,input_dim):\n",
    "        super(MyLayer,self).__init__()\n",
    "         #inicializa pesos y no se entrena\n",
    "        self.w=self.add_weight(shape=(input_dim,units),initializer='random_normal',trainable=False)\n",
    "        #inicializa sesgos y no se entrena\n",
    "        self.b=self.add_weight(shape=(units,),initializer='zeros',trainable=False) \n",
    "    \n",
    "    def call(self,inputs):\n",
    "        return tf.matmul(inputs,self.w)+self.b\n",
    "    \n",
    "my_layer=MyLayer(3,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable weights: 0\n",
      "non-trainable weights: 2\n"
     ]
    }
   ],
   "source": [
    "#hacemos que ninguno sea entrenable ni sesgos ni pesos se entrenaran\n",
    "print('trainable weights:', len(my_layer.trainable_weights))\n",
    "print('non-trainable weights:', len(my_layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom layer to accumulate means of output values\n",
    "\n",
    "class MyLayerMean(Layer):\n",
    "         \n",
    "    def __init__(self,units,input_dim):\n",
    "        super(MyLayerMean,self).__init__()\n",
    "        #inicializa pesos \n",
    "        self.w=self.add_weight(shape=(input_dim,units),initializer='random_normal')\n",
    "        #inicializa sesgos\n",
    "        self.b=self.add_weight(shape=(units,),initializer='zeros')\n",
    "        #inicializa suna variable en 0 y no se entrena\n",
    "        self.sum_activation=tf.Variable(initial_value=tf.zeros((units,)),trainable=False)\n",
    "        #inicializa suna variable en 0 y no se entrena\n",
    "        self.number_call=tf.Variable(initial_value=0,trainable=False)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        activations=tf.matmul(inputs,self.w)+self.b\n",
    "        self.sum_activation.assign_add(tf.reduce_sum(activations,axis=0))#suma resultados\n",
    "        self.number_call.assign_add(inputs.shape[0])#suma numero de entradas\n",
    "        return activations, self.sum_activation/tf.cast(self.number_call,tf.float32)\n",
    "    \n",
    "my_layer=MyLayerMean(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.09079675 -0.15304194 -0.11420681]\n",
      "[-0.09079675 -0.15304194 -0.11420681]\n"
     ]
    }
   ],
   "source": [
    "# Test the layer\n",
    "\n",
    "y , activation_means = my_layer(tf.ones((1, 5)))\n",
    "print(activation_means.numpy())\n",
    "\n",
    "y , activation_means = my_layer(tf.ones((1, 5)))\n",
    "print(activation_means.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dropout layer as a custom layer\n",
    "\n",
    "class MyDropout(Layer):\n",
    "\n",
    "    def __init__(self, rate):\n",
    "        super(MyDropout, self).__init__()\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Define forward pass for dropout layer\n",
    "        return tf.nn.dropout(inputs,rate=self.rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the custom layers into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model using custom layers with the model subclassing API\n",
    "\n",
    "class MyModel(Model):\n",
    "\n",
    "    def __init__(self, units_1, input_dim_1, units_2, units_3):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Define layers\n",
    "        self.layer1=MyLayer(units_1,input_dim_1)\n",
    "        self.dropout1=MyDropout(0.5)\n",
    "        self.layer2=MyLayer(units_2,units_1)\n",
    "        self.dropout2=MyDropout(0.5)\n",
    "        self.layer3=MyLayer(units_3,units_2)\n",
    "        self.softmax=Softmax()\n",
    "    def call(self, inputs):\n",
    "        # Define forward pass\n",
    "        x=self.layer1(inputs)\n",
    "        x=tf.nn.relu(x)\n",
    "        x=self.dropout1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=tf.nn.relu(x)\n",
    "        x=self.dropout2(x)\n",
    "        x=self.layer3(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.0151086  0.01759894 0.01634271 0.00543772 0.00947135 0.08832284\n",
      "  0.01350588 0.02503939 0.04355713 0.01264055 0.02217345 0.00959837\n",
      "  0.02580766 0.01230905 0.01986783 0.00677113 0.0500555  0.05807772\n",
      "  0.03461907 0.02128311 0.00666977 0.0120061  0.01245167 0.01080156\n",
      "  0.01927049 0.01774717 0.00541066 0.01086818 0.01693503 0.01732861\n",
      "  0.0462113  0.01070863 0.02325813 0.05351608 0.01414227 0.00546212\n",
      "  0.02204781 0.03332924 0.01831672 0.01247305 0.0210377  0.01622159\n",
      "  0.0224103  0.01140356 0.02535181 0.02703239]], shape=(1, 46), dtype=float32)\n",
      "Model: \"my_model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "my_layer_14 (MyLayer)        multiple                  640064    \n",
      "_________________________________________________________________\n",
      "my_dropout_2 (MyDropout)     multiple                  0         \n",
      "_________________________________________________________________\n",
      "my_layer_15 (MyLayer)        multiple                  4160      \n",
      "_________________________________________________________________\n",
      "my_dropout_3 (MyDropout)     multiple                  0         \n",
      "_________________________________________________________________\n",
      "my_layer_16 (MyLayer)        multiple                  2990      \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 647,214\n",
      "Trainable params: 0\n",
      "Non-trainable params: 647,214\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a model object\n",
    "\n",
    "model = MyModel(64,10000,64,46)\n",
    "print(model(tf.ones((1, 10000))))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_3\"></a>\n",
    "## Automatic differentiation\n",
    "\n",
    "En tensorflow podemos calcular derivadas de forma automatica lo cual es muy util a la hora de hacer optimizaciones\n",
    "de funciones costo, esto nos va a servir para poder personalizar nuestos ciclos de entrenamiento de una forma mas\n",
    "granular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#importamos tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#creamos valor donde se evalua la derivada ya que no calcula derivadas simbolicas si no numericas\n",
    "x=tf.constant(2.0)\n",
    "\n",
    "#con esa funcionalidad tensorflow recuerda la grafica de las operaciones realizadas\n",
    "with tf.GradientTape() as tape:\n",
    "    #especificamos que se fije en las operaciones sobre x\n",
    "    tape.watch(x)\n",
    "    #hacemos operaciones sobre x\n",
    "    y=x**2\n",
    "    #indicamos que derive y respecto x dy/dx\n",
    "    grad=tape.gradient(y,x)\n",
    "\n",
    "#dy/dx=2x en x=2.0 es 4.0\n",
    "print(grad)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.13673721, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#podemos crear derivadas mas complejas usando regla de cadena incluso\n",
    "import tensorflow as tf\n",
    "\n",
    "#tenemos 4 valores para x\n",
    "x=tf.constant([0,1,2,3],dtype=tf.float32)\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    #eleva los 4 valores al cuadrado y los suma\n",
    "    y=tf.reduce_sum(x**2)\n",
    "    #al resultado anterior le aplica coseno\n",
    "    z=tf.math.sin(y)\n",
    "    #calcula derivada de dz/dy\n",
    "    dz_dy=tape.gradient(z,y)\n",
    "\n",
    "print(dz_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.13673721, shape=(), dtype=float32)\n",
      "\n",
      "tf.Tensor([0.         0.27347443 0.54694885 0.82042325], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#podemos calcular derivadas respecto a distintas variables a la vez\n",
    "#podemos crear derivadas mas complejas usando regla de cadena incluso\n",
    "import tensorflow as tf\n",
    "\n",
    "#tenemos 4 valores para x\n",
    "x=tf.constant([0,1,2,3],dtype=tf.float32)\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    #eleva los 4 valores al cuadrado y los suma\n",
    "    y=tf.reduce_sum(x**2)\n",
    "    #al resultado anterior le aplica coseno\n",
    "    z=tf.math.sin(y)\n",
    "    #calcula derivada de dz/dy\n",
    "    dz_dy, dz_dx=tape.gradient(z,[y,x])\n",
    "#es un escalar\n",
    "print(dz_dy)\n",
    "print()\n",
    "#es una lista porque deriva respecto a cada x y tenemos 4 valores distintos de x\n",
    "print(dz_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7fd4672a90>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create data from a noise contaminated linear model\n",
    "\n",
    "def MakeNoisyData(m, b, n=20):\n",
    "    x = tf.random.uniform(shape=(n,))\n",
    "    noise = tf.random.normal(shape=(len(x),), stddev=0.1)\n",
    "    y = m * x + b + noise\n",
    "    return x, y\n",
    "\n",
    "m=1\n",
    "b=2\n",
    "x_train, y_train = MakeNoisyData(m,b)\n",
    "plt.plot(x_train, y_train, 'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.03133849 0.03525284 0.02880232 0.02846309 0.00946849 0.03884461\n",
      " 0.03066305 0.02011103 0.02011726 0.03689848 0.03902383 0.03815815\n",
      " 0.03459287 0.01059877 0.00117222 0.03282144 0.01576653 0.02971289\n",
      " 0.03916461 0.0035168 ], shape=(20,), dtype=float32)\n",
      "\n",
      "[<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.03978279], dtype=float32)>, <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# Build a custom layer for the linear regression model\n",
    "class LinearLayer(Layer):\n",
    "    def __init__(self):\n",
    "        super(LinearLayer,self).__init__()\n",
    "        self.m=self.add_weight(shape=(1,),initializer='random_normal')\n",
    "        self.b=self.add_weight(shape=(1,),initializer='zeros')\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        return self.m*inputs+self.b\n",
    "\n",
    "linear_regression=LinearLayer()\n",
    "print(linear_regression(x_train))\n",
    "print()\n",
    "print(linear_regression.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loss 7.154012\n"
     ]
    }
   ],
   "source": [
    "# Define the mean squared error loss function\n",
    "\n",
    "def SquaredError(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true)) \n",
    "\n",
    "starting_loss = SquaredError(linear_regression(x_train), y_train)\n",
    "print(\"Starting loss\", starting_loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 0, Loss 7.154012\n",
      "Paso 1, Loss 5.216798\n",
      "Paso 2, Loss 3.804993\n",
      "Paso 3, Loss 2.776094\n",
      "Paso 4, Loss 2.026247\n",
      "Paso 5, Loss 1.479766\n",
      "Paso 6, Loss 1.081494\n",
      "Paso 7, Loss 0.791232\n",
      "Paso 8, Loss 0.579685\n",
      "Paso 9, Loss 0.425505\n",
      "Paso 10, Loss 0.313131\n",
      "Paso 11, Loss 0.231226\n",
      "Paso 12, Loss 0.171525\n",
      "Paso 13, Loss 0.128006\n",
      "Paso 14, Loss 0.096280\n",
      "Paso 15, Loss 0.073149\n",
      "Paso 16, Loss 0.056282\n",
      "Paso 17, Loss 0.043980\n",
      "Paso 18, Loss 0.035005\n",
      "Paso 19, Loss 0.028454\n",
      "Paso 20, Loss 0.023671\n",
      "Paso 21, Loss 0.020176\n",
      "Paso 22, Loss 0.017619\n",
      "Paso 23, Loss 0.015747\n",
      "Paso 24, Loss 0.014374\n"
     ]
    }
   ],
   "source": [
    "# Implement a gradient descent training loop for the linear regression model\n",
    "\n",
    "#gardiente desendente hecho a mano\n",
    "learning_rate=0.05\n",
    "steps=25\n",
    "\n",
    "for i in range(steps):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions=linear_regression(x_train)\n",
    "        #nota como no hacemos operaciones si no pasamos funcion y no definimos el watch\n",
    "        loss=SquaredError(predictions,y_train)\n",
    "    gradients=tape.gradient(loss,linear_regression.trainable_variables)\n",
    "    \n",
    "    linear_regression.m.assign_sub(learning_rate*gradients[0])\n",
    "    linear_regression.b.assign_sub(learning_rate*gradients[1])\n",
    "    \n",
    "    print(\"Paso %d, Loss %f\" % (i,loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m:1,  trained m:[1.256899]\n",
      "b:2,  trained b:[1.7933115]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7fd4548390>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learned regression model\n",
    "\n",
    "print(\"m:{},  trained m:{}\".format(m,linear_regression.m.numpy()))\n",
    "print(\"b:{},  trained b:{}\".format(b,linear_regression.b.numpy()))\n",
    "\n",
    "plt.plot(x_train, y_train, 'b.')\n",
    "\n",
    "x_linear_regression=np.linspace(min(x_train), max(x_train),50)\n",
    "plt.plot(x_linear_regression, linear_regression.m*x_linear_regression+linear_regression.b, 'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_4\"></a>\n",
    "## Custom training loops\n",
    "\n",
    "Ya tenemos todos los elementos para poder genrar nuestros propios ciclos de entrenamiento, manejando paso por paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ya hemos visto como hacer nuestro algoritmo de optimizacion paso por paso\n",
    "#tambien podemos usar una instancia de un objeto de funcion de perdida\n",
    "# y una instancia de optimizador al que le pasamos nuestros gradientes para no programar nostros mismos\n",
    "#el algoritmo de optimizacion por ejemplo\n",
    "\n",
    "#importamos tensorflow\n",
    "import tensorflow as tf\n",
    "#importsamos la funcion de perdida que ocuparemos\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "#importamos el algoritmo de optimizacion que usaremos\n",
    "from tensorflow.keras.optimizer import SDG\n",
    "\n",
    "#instanciando un modelo \n",
    "my_model=MyModel()\n",
    "\n",
    "#instanciamos nuestra perdida y el algoritmo\n",
    "loss=MeanSquaredError()\n",
    "optimizer=SDG(learning_rate=0.05,momentum=0.9)\n",
    "\n",
    "#derivamos funcion de perdida usando GradientTape\n",
    "with GradientTape() as tape:\n",
    "    #aplicamos perdida a la prediccion del modelo y lo esperado\n",
    "    current_loss=loss(my_model(inputs),outputs)\n",
    "    #calculamos los gradientes respecto a cda variable entrenada y los guardamos en una lista\n",
    "    grads=tape.gradient(current_loss,my_model.trainable_variables)\n",
    "    \n",
    "#aplicamos el optimizador a los gradientes calculados con ayuda del metodo .apply_gradients\n",
    "#eso actualizara automaticamente los parametros de acuerdo al optimizador \n",
    "optimizer.apply_gradients(zip(grads,my_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#podemos aplicar tambien el procedimiento por lotes\n",
    "# supongamos que training_dataset es un objeto dataset que regresa lotes de inputs y outputs as√≠\n",
    "\n",
    "#importamos tensorflow\n",
    "import tensorflow as tf\n",
    "#importsamos la funcion de perdida que ocuparemos\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "#importamos el algoritmo de optimizacion que usaremos\n",
    "from tensorflow.keras.optimizer import SDG\n",
    "\n",
    "#instanciando un modelo \n",
    "my_model=MyModel()\n",
    "\n",
    "#instanciamos nuestra perdida y el algoritmo\n",
    "loss=MeanSquaredError()\n",
    "optimizer=SDG(learning_rate=0.05,momentum=0.9)\n",
    "\n",
    "#note que esto es una sola epoca de entrenamiento\n",
    "#suponemos que suponemos que training_dataset es finito se recorre una vez y se acaba con la ejecucion\n",
    "\n",
    "#aqui guardaremos las perdidas del lote\n",
    "batch_losses=[]\n",
    "\n",
    "#recorremos el lote\n",
    "for inputs,outputs in training_dataset:\n",
    "    \n",
    "    #derivamos funcion de perdida usando GradientTape\n",
    "    with GradientTape() as tape:\n",
    "        #aplicamos perdida a la prediccion del modelo y lo esperado\n",
    "        current_loss=loss(my_model(inputs),outputs)\n",
    "        #calculamos los gradientes respecto a cda variable entrenada y los guardamos en una lista\n",
    "        grads=tape.gradient(current_loss,my_model.trainable_variables)\n",
    "    \n",
    "    #guardamos la perdida por batch\n",
    "    batch_losses.append(current_loss)\n",
    "    #aplicamos el optimizador a los gradientes calculados con ayuda del metodo .apply_gradients\n",
    "    #eso actualizara automaticamente los parametros de acuerdo al optimizador \n",
    "    optimizer.apply_gradients(zip(grads,my_model.trainable_variables))\n",
    "    \n",
    "    \n",
    "#para hacer el proceo anterior por epoca basta meterlo en un ciclo de epocas\n",
    "\n",
    "#aqui guardaremos tambien la perdida de cada epoca\n",
    "epoch_losses=[]\n",
    "for epoch in range(num_epochs):\n",
    "    #aqui guardaremos las perdidas del lote\n",
    "    batch_losses=[]\n",
    "\n",
    "    #recorremos el lote\n",
    "    for inputs,outputs in training_dataset:\n",
    "\n",
    "        #derivamos funcion de perdida usando GradientTape\n",
    "        with GradientTape() as tape:\n",
    "            #aplicamos perdida a la prediccion del modelo y lo esperado\n",
    "            current_loss=loss(my_model(inputs),outputs)\n",
    "            #calculamos los gradientes respecto a cda variable entrenada y los guardamos en una lista\n",
    "            grads=tape.gradient(current_loss,my_model.trainable_variables)\n",
    "\n",
    "        #guardamos la perdida por batch\n",
    "        batch_losses.append(current_loss)\n",
    "        #aplicamos el optimizador a los gradientes calculados con ayuda del metodo .apply_gradients\n",
    "        #eso actualizara automaticamente los parametros de acuerdo al optimizador \n",
    "        optimizer.apply_gradients(zip(grads,my_model.trainable_variables))\n",
    "    \n",
    "    #guardamos por cada epoca la media de las perdidas de los lotes\n",
    "    epoch_losses.append(np.mean(batch_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom layers and model\n",
    "class MyLayer(Layer):\n",
    "         \n",
    "    def __init__(self,units):\n",
    "        super(MyLayer,self).__init__()\n",
    "        self.units=units\n",
    "    #aqui separamos el build del init\n",
    "    def build(self,input_shape):\n",
    "        self.w=self.add_weight(shape=(input_shape[-1],self.units),initializer='random_normal',name='kernel') #inicializa pesos   \n",
    "        self.b=self.add_weight(shape=(self.units,),initializer='zeros',name='bias') #inicializa sesgos\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        return tf.matmul(inputs,self.w)+self.b\n",
    "\n",
    "class MyDropout(Layer):\n",
    "\n",
    "    def __init__(self, rate):\n",
    "        super(MyDropout, self).__init__()\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Define forward pass for dropout layer\n",
    "        return tf.nn.dropout(inputs,rate=self.rate)\n",
    "    \n",
    "class MyModel(Model):\n",
    "\n",
    "    def __init__(self, units_1, units_2, units_3):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Define layers\n",
    "        self.layer1=MyLayer(units_1)\n",
    "        self.dropout1=MyDropout(0.5)\n",
    "        self.layer2=MyLayer(units_2)\n",
    "        self.dropout2=MyDropout(0.5)\n",
    "        self.layer3=MyLayer(units_3)\n",
    "        self.softmax=Softmax()\n",
    "    def call(self, inputs):\n",
    "        # Define forward pass\n",
    "        x=self.layer1(inputs)\n",
    "        x=tf.nn.relu(x)\n",
    "        x=self.dropout1(x)\n",
    "        x=self.layer2(x)\n",
    "        x=tf.nn.relu(x)\n",
    "        x=self.dropout2(x)\n",
    "        x=self.layer3(x)\n",
    "        return self.softmax(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.00910492 0.01305458 0.02417463 0.00952883 0.01373023 0.0145389\n",
      "  0.01506997 0.01002813 0.0136742  0.01921157 0.0192055  0.02141683\n",
      "  0.01661855 0.010644   0.01567224 0.00970886 0.00484321 0.01452804\n",
      "  0.03031491 0.00679364 0.00895576 0.01273956 0.01165674 0.00666535\n",
      "  0.02038888 0.00980087 0.02776306 0.00392505 0.00779932 0.04076068\n",
      "  0.00775666 0.02060031 0.01959355 0.00970403 0.03932459 0.00674915\n",
      "  0.01185679 0.01237496 0.01008896 0.01033838 0.0100258  0.03127175\n",
      "  0.01975719 0.020266   0.00611509 0.01311046 0.01773734 0.00826315\n",
      "  0.0123423  0.0165279  0.01596126 0.00593764 0.03398729 0.01364162\n",
      "  0.01197923 0.0202081  0.01037764 0.03080892 0.01225139 0.02955383\n",
      "  0.01738879 0.01678987 0.013521   0.02147209]], shape=(1, 64), dtype=float32)\n",
      "Model: \"my_model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "my_layer_24 (MyLayer)        multiple                  640064    \n",
      "_________________________________________________________________\n",
      "my_dropout_9 (MyDropout)     multiple                  0         \n",
      "_________________________________________________________________\n",
      "my_layer_25 (MyLayer)        multiple                  4160      \n",
      "_________________________________________________________________\n",
      "my_dropout_10 (MyDropout)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "my_layer_26 (MyLayer)        multiple                  4160      \n",
      "_________________________________________________________________\n",
      "softmax_4 (Softmax)          multiple                  0         \n",
      "=================================================================\n",
      "Total params: 648,384\n",
      "Trainable params: 648,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#instanciate the model object\n",
    "model=MyModel(64,64,64)\n",
    "print(model(tf.ones((1,10000))))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the reuters dataset and define the class_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "from tensorflow.keras.datasets import reuters\n",
    "\n",
    "#contiene ejemplos de textos que pertenecen a topicos\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
    "\n",
    "class_names = ['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',\n",
    "   'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',\n",
    "   'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',\n",
    "   'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',\n",
    "   'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: earn\n"
     ]
    }
   ],
   "source": [
    "# Print the class of the first sample\n",
    "\n",
    "print(\"Label: {}\".format(class_names[train_labels[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the dataset word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Reuters word index\n",
    "\n",
    "word_to_index = reuters.get_word_index()\n",
    "\n",
    "invert_word_index = dict([(value, key) for (key, value) in word_to_index.items()])\n",
    "text_news = ' '.join([invert_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
     ]
    }
   ],
   "source": [
    "# Print the first data example sentence\n",
    "print(text_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (8982, 10000)\n",
      "Shape of x_test: (2246, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Define a function that encodes the data into a 'bag of words' representation\n",
    "\n",
    "def bag_of_words(text_samples, elements=10000):\n",
    "    output = np.zeros((len(text_samples), elements))\n",
    "    for i, word in enumerate(text_samples):\n",
    "        output[i, word] = 1.\n",
    "    return output\n",
    "\n",
    "x_train = bag_of_words(train_data)\n",
    "x_test = bag_of_words(test_data)\n",
    "\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of x_test:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss function and optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical cross entropy loss and Adam optimizer\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def loss(model, x, y, wd):\n",
    "    kernel_variables = []\n",
    "    for l in model.layers:\n",
    "        for w in l.weights:\n",
    "            if 'kernel' in w.name:\n",
    "                kernel_variables.append(w)\n",
    "    wd_penalty = wd * tf.reduce_sum([tf.reduce_sum(tf.square(k)) for k in kernel_variables])\n",
    "    y_ = model(x)\n",
    "    return loss_object(y_true=y, y_pred=y_) + wd_penalty\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the forward and backward pass\n",
    "\n",
    "def grad(model, inputs, targets, wd):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, wd)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer my_model_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 000: Loss: 3.397, Accuracy: 48.820%\n",
      "Epoch 001: Loss: 1.958, Accuracy: 61.178%\n",
      "Epoch 002: Loss: 1.877, Accuracy: 64.919%\n",
      "Epoch 003: Loss: 1.825, Accuracy: 66.956%\n",
      "Epoch 004: Loss: 1.799, Accuracy: 68.481%\n",
      "Epoch 005: Loss: 1.776, Accuracy: 69.717%\n",
      "Epoch 006: Loss: 1.755, Accuracy: 69.595%\n",
      "Epoch 007: Loss: 1.736, Accuracy: 69.639%\n",
      "Epoch 008: Loss: 1.732, Accuracy: 70.107%\n",
      "Epoch 009: Loss: 1.719, Accuracy: 70.953%\n",
      "Duration :270.353\n"
     ]
    }
   ],
   "source": [
    "# Implement the training loop\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "start_time = time.time()\n",
    "#Formamos objeto data set\n",
    "train_dataset=tf.data.Dataset.from_tensor_slices((x_train,train_labels))\n",
    "#Formamos lotes\n",
    "train_dataset=train_dataset.batch(32)\n",
    "\n",
    "#datos para graficar\n",
    "train_loss_results=[]\n",
    "train_accuracy_results=[]\n",
    "\n",
    "weight_decay=0.005\n",
    "num_epochs=10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg=tf.keras.metrics.Mean()\n",
    "    epoch_accuracy=tf.keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    #ciclo de entrenamiento\n",
    "    for x,y in train_dataset:\n",
    "        loss_value,grads=grad(model,x,y,weight_decay)\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "        \n",
    "        #calulando perdida actual\n",
    "        epoch_loss_avg(loss_value)\n",
    "        #comparando etiqueta predicha con la actual\n",
    "        epoch_accuracy(to_categorical(y),model(x))\n",
    "    \n",
    "    #final de epoca\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "    \n",
    "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))\n",
    "    \n",
    "print(\"Duration :{:.3f}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset object for the test set\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, test_labels))\n",
    "test_dataset = test_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect average loss and accuracy\n",
    "\n",
    "epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.835\n",
      "Test accuracy: 67.453%\n"
     ]
    }
   ],
   "source": [
    "# Loop over the test set and print scores\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "for x, y in test_dataset:\n",
    "    # Optimize the model\n",
    "    loss_value = loss(model, x, y, weight_decay)    \n",
    "    # Compute current loss\n",
    "    epoch_loss_avg(loss_value)  \n",
    "    # Compare predicted label to actual label\n",
    "    epoch_accuracy(to_categorical(y), model(x))\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(epoch_loss_avg.result().numpy()))\n",
    "print(\"Test accuracy: {:.3%}\".format(epoch_accuracy.result().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training loss and accuracy\n",
    "\n",
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(train_accuracy_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: earn\n",
      "     Label: earn\n"
     ]
    }
   ],
   "source": [
    "# Get the model prediction for an example input\n",
    "\n",
    "predicted_label = np.argmax(model(x_train[np.newaxis,0]),axis=1)[0]\n",
    "print(\"Prediction: {}\".format(class_names[predicted_label]))\n",
    "print(\"     Label: {}\".format(class_names[train_labels[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_5\"></a>\n",
    "## tf.function decorator\n",
    "\n",
    "Tensorflow tiene dos formas de calcular las cosas usando graficas y mediante codigo que ejecuta en tiempo de ejecucion (eager) que es la que se hace por default, los graficos no reducen calculos hacen un mapeo mediante graficas de ellos, hacer codigos para las graficas es complejo pero tensorflow tiene una forma de indicar que partes de codigo se interpretan como graficas y tensorflow en automatico contruye la grafica por detras esto es util pues este metodo hace que los calculos sean mucho mas r√°pidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esto es una funcion comun y corriente que se ejecuta al momento eager\n",
    "def f (x):\n",
    "    #procesa x\n",
    "    return x\n",
    "\n",
    "#si yo decoro mi funcion con @tf.function la funcion se ve igual y se ejecuta igual pero por detras tensorflow\n",
    "#construye la grafica de calculos que se usa para procesar x\n",
    "#cada que mandes a llamar esta funcion se ejecutara como grafica\n",
    "@tf.function\n",
    "def f (x):\n",
    "    #procesa x\n",
    "    return x\n",
    "\n",
    "#si ejecutas muestra la grafica que tensorflow genera por detras puedes ver como el codigo es mas complejo\n",
    "print(tf.autograph.to_code(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Softmax\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import reuters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new model\n",
    "#usamos el modelo que definimos en la seccion anterior\n",
    "model=MyModel(64,64,46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redefine the grad function using the @tf.function decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the @tf.function decorator\n",
    "\n",
    "#la misma funcion que hace rato ahora la interpretamos generando su grafica automatica detras de escena\n",
    "@tf.function\n",
    "def grad(model, inputs, targets, wd):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, wd)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer my_model_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 000: Loss: 3.293, Accuracy: 48.720%\n",
      "Epoch 001: Loss: 1.913, Accuracy: 59.797%\n",
      "Epoch 002: Loss: 1.828, Accuracy: 65.153%\n",
      "Epoch 003: Loss: 1.779, Accuracy: 67.502%\n",
      "Epoch 004: Loss: 1.762, Accuracy: 68.504%\n",
      "Epoch 005: Loss: 1.748, Accuracy: 69.205%\n",
      "Epoch 006: Loss: 1.732, Accuracy: 69.383%\n",
      "Epoch 007: Loss: 1.722, Accuracy: 70.085%\n",
      "Epoch 008: Loss: 1.711, Accuracy: 70.396%\n",
      "Epoch 009: Loss: 1.695, Accuracy: 70.730%\n",
      "Duration :222.034\n"
     ]
    }
   ],
   "source": [
    "# Re-run the training loop\n",
    "#pongamos el mismo ciclo de entrenamiento que codificamos anteriormente no hay qu modificarle nada \n",
    "#veamos como simplemente agregando el decorador como mejora el desempe√±o\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "start_time = time.time()\n",
    "#Formamos objeto data set\n",
    "train_dataset=tf.data.Dataset.from_tensor_slices((x_train,train_labels))\n",
    "#Formamos lotes\n",
    "train_dataset=train_dataset.batch(32)\n",
    "\n",
    "#datos para graficar\n",
    "train_loss_results=[]\n",
    "train_accuracy_results=[]\n",
    "\n",
    "weight_decay=0.005\n",
    "num_epochs=10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg=tf.keras.metrics.Mean()\n",
    "    epoch_accuracy=tf.keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    #ciclo de entrenamiento\n",
    "    for x,y in train_dataset:\n",
    "        loss_value,grads=grad(model,x,y,weight_decay)\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "        \n",
    "        #calulando perdida actual\n",
    "        epoch_loss_avg(loss_value)\n",
    "        #comparando etiqueta predicha con la actual\n",
    "        epoch_accuracy(to_categorical(y),model(x))\n",
    "    \n",
    "    #final de epoca\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "    \n",
    "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))\n",
    "    \n",
    "print(\"Duration :{:.3f}\".format(time.time() - start_time))\n",
    "\n",
    "#anteriormente duro 270.353 la ejecucion\n",
    "#con el decorador solo tarda 222.034"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the autograph code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def tf__grad(model, inputs, targets, wd):\n",
      "  do_return = False\n",
      "  retval_ = ag__.UndefinedReturnValue()\n",
      "  with ag__.FunctionScope('grad', 'grad_scope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as grad_scope:\n",
      "    with tf.GradientTape() as tape:\n",
      "      loss_value = ag__.converted_call(loss, grad_scope.callopts, (model, inputs, targets, wd), None, grad_scope)\n",
      "    do_return = True\n",
      "    retval_ = grad_scope.mark_return_value((loss_value, ag__.converted_call(tape.gradient, grad_scope.callopts, (loss_value, model.trainable_variables), None, grad_scope)))\n",
      "  do_return,\n",
      "  return ag__.retval(retval_)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use tf.autograph.to_code to see the generated code\n",
    "#veamos la grafica de la funcion que se auto genero\n",
    "print(tf.autograph.to_code(grad.python_function))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
