{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kk4781vq9qs3"
   },
   "source": [
    "## Neural translation model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0Xk88KM9qs6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import unicodedata\n",
    "import re\n",
    "from IPython.display import Image\n",
    "# for splitting sentences \n",
    "import re\n",
    "# for finding random samples\n",
    "import random\n",
    "from tensorflow.keras.layers import Layer, concatenate, Input, Masking, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqnTHWhi9qs8"
   },
   "source": [
    "We will use a language dataset from http://www.manythings.org/anki/ to build a neural translation model. This dataset consists of over 200,000 pairs of sentences in English and German. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADM9a45K9qs-"
   },
   "outputs": [],
   "source": [
    "#  load the dataset\n",
    "\n",
    "NUM_EXAMPLES = 20000\n",
    "data_examples = []\n",
    "with open('data/deu.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        if len(data_examples) < NUM_EXAMPLES:\n",
    "            data_examples.append(line)\n",
    "        else:\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2LXRHLu9qs_"
   },
   "outputs": [],
   "source": [
    "# preprocess English and German sentences\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"ü\", 'ue', sentence)\n",
    "    sentence = re.sub(r\"ä\", 'ae', sentence)\n",
    "    sentence = re.sub(r\"ö\", 'oe', sentence)\n",
    "    sentence = re.sub(r'ß', 'ss', sentence)\n",
    "    \n",
    "    sentence = unicode_to_ascii(sentence)\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r\"[^a-z?.!,']+\", \" \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUAxZjdp9qtB"
   },
   "source": [
    "#### The custom translation model\n",
    "\n",
    "The custom model consists of an encoder RNN and a decoder RNN. The encoder takes words of an English sentence as input, and uses a pre-trained word embedding to embed the words into a 128-dimensional space. To indicate the end of the input sentence, a special end token (in the same 128-dimensional space) is passed in as an input. This token is a TensorFlow Variable that is learned in the training phase (unlike the pre-trained word embedding, which is frozen).\n",
    "\n",
    "The decoder RNN takes the internal state of the encoder network as its initial state. A start token is passed in as the first input, which is embedded using a learned German word embedding. The decoder RNN then makes a prediction for the next German word, which during inference is then passed in as the following input, and this process is repeated until the special `<end>` token is emitted from the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJhHFQQ49qtC"
   },
   "source": [
    "## 1. Text preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osiJnSUy9qtE",
    "outputId": "cf731217-9d3e-445f-88a6-7a848c91ab96",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:\t\twho has it now ?\n",
      "German sentence:\t\t<start> wer hat ihn jetzt ? <end>\n",
      "German sentence sequence:\t[1, 43, 16, 44, 62, 7, 2]\n",
      "\n",
      "English sentence:\t\ttom was here .\n",
      "German sentence:\t\t<start> tom war hier . <end>\n",
      "German sentence sequence:\t[1, 5, 24, 33, 3, 2]\n",
      "\n",
      "English sentence:\t\ttom will miss me .\n",
      "German sentence:\t\t<start> tom wird mich vermissen . <end>\n",
      "German sentence sequence:\t[1, 5, 48, 22, 473, 3, 2]\n",
      "\n",
      "English sentence:\t\ti'm shy .\n",
      "German sentence:\t\t<start> ich bin schuechtern . <end>\n",
      "German sentence sequence:\t[1, 4, 15, 337, 3, 2]\n",
      "\n",
      "English sentence:\t\ti can't eat .\n",
      "German sentence:\t\t<start> ich kann nicht essen . <end>\n",
      "German sentence sequence:\t[1, 4, 30, 12, 154, 3, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_sentences = [preprocess_sentence(example.split('\\t')[0]) for example in data_examples]\n",
    "de_sentences = [preprocess_sentence(example.split('\\t')[1]) for example in data_examples]\n",
    "de_sentences = [\"<start> \" + sentence + \" <end>\" for sentence in de_sentences]\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None,\n",
    "                                                  filters='',\n",
    "                                                  lower=True)\n",
    "tokenizer.fit_on_texts(de_sentences)\n",
    "de_sentences_sequences = tokenizer.texts_to_sequences(de_sentences)\n",
    "random_indices = np.random.choice(len(de_sentences), size=5, replace=False)\n",
    "for idx in random_indices:\n",
    "    print(f\"English sentence:\\t\\t{en_sentences[idx]}\")\n",
    "    print(f\"German sentence:\\t\\t{de_sentences[idx]}\")\n",
    "    print(f\"German sentence sequence:\\t{de_sentences_sequences[idx]}\\n\")\n",
    "\n",
    "# Padding\n",
    "de_sentences_sequences = tf.keras.preprocessing.sequence.pad_sequences(de_sentences_sequences, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysm6Qz8y9qtM"
   },
   "source": [
    "## 2. Prepare the data with tf.data.Dataset objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKWmnBk39qtM"
   },
   "source": [
    "#### Load the embedding layer\n",
    "As part of the dataset preproceessing ,  use a pre-trained English word embedding module from TensorFlow Hub. The URL for the module is https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1. This module has also been made available as a complete saved model in the folder `'./models/tf2-preview_nnlm-en-dim128_1'`. \n",
    "\n",
    "This embedding takes a batch of text tokens in a 1-D tensor of strings as input. It then embeds the separate tokens into a 128-dimensional space. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOi37qSb9qtN"
   },
   "outputs": [],
   "source": [
    "# Load embedding module from Tensorflow Hub\n",
    "\n",
    "embedding_layer = hub.KerasLayer(\"./models/tf2-preview_nnlm-en-dim128_1\", \n",
    "                                 output_shape=[128], input_shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epYm0bIE9qtN",
    "outputId": "40436ee9-ca94-46ec-e331-8b39717fe146"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([7, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the layer\n",
    "\n",
    "embedding_layer(tf.constant([\"these\", \"aren't\", \"the\", \"droids\", \"you're\", \"looking\", \"for\"])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6O5tS309qtP"
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "en_train, en_val, de_train, de_val = train_test_split(en_sentences, de_sentences_sequences, test_size=0.2)\n",
    "# Create train and validation Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((en_train, de_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((en_val, de_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AzaKy4r9qtQ",
    "outputId": "c4f06a12-3992-4016-cd01-974f19476af9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(16, None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(16, 14), dtype=tf.int32, name=None))\n",
      "(TensorSpec(shape=(16, None, 128), dtype=tf.float32, name=None), TensorSpec(shape=(16, 14), dtype=tf.int32, name=None))\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    def map_whitespace_split(en, de):\n",
    "        return tf.strings.split(en, sep=\" \"), de\n",
    "    \n",
    "    def map_embed_sentences(en ,de):\n",
    "        return embedding_layer(en), de\n",
    "    \n",
    "    def filter_cut(en, de):\n",
    "        return tf.math.less(tf.cast(tf.shape(en)[0],dtype=tf.int32), 13)\n",
    "    \n",
    "    def map_pad_sentences(en, de):\n",
    "        padding_needed = [[13 - len(en), 0], [0, 0]]\n",
    "        return tf.pad(en, padding_needed), de\n",
    "    \n",
    "    dataset = dataset.map(map_whitespace_split)\n",
    "    dataset = dataset.map(map_embed_sentences)\n",
    "    dataset = dataset.filter(filter_cut)\n",
    "    dataset = dataset.map(map_pad_sentences)\n",
    "    dataset = dataset.batch(16, drop_remainder=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_dataset = preprocess_dataset(train_dataset)\n",
    "val_dataset = preprocess_dataset(val_dataset)\n",
    "# Print the element spec for each Dataset\n",
    "print(train_dataset.element_spec)\n",
    "print(val_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhYVvCIm9qtR",
    "outputId": "46e1068b-c708-4750-e270-e9715a7ae968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the English data example from the training Dataset: (16, 13, 128)\n",
      "German data example Tensor from the validation Dataset: (16, 14)\n"
     ]
    }
   ],
   "source": [
    "en, de = next(iter(train_dataset.take(1)))\n",
    "print(f\"Shape of the English data example from the training Dataset: {en.shape}\")\n",
    "print(f\"German data example Tensor from the validation Dataset: {de.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xVZJBCj9qtS"
   },
   "source": [
    "## 3. Create the custom layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Almh8QV9qtT"
   },
   "outputs": [],
   "source": [
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomLayer, self).__init__(**kwargs)\n",
    "        self.token_embedding = tf.Variable(initial_value=tf.random.uniform(shape=(128,)), trainable=True)\n",
    "\n",
    "    def call(self, input):\n",
    "        output = tf.tile(tf.reshape(self.token_embedding, shape=(1, 1, tf.shape(self.token_embedding)[0])),\n",
    "                         [tf.shape(input)[0], 1, 1])\n",
    "        return tf.keras.layers.concatenate([input, output], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U22oCSMR9qtU",
    "outputId": "f6d684ba-b908-4499-dac8-dad8c0a914e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape before the end token embedding: (16, 13, 128)\n",
      "Batch shape after the end token embedding: (16, 14, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Batch shape before the end token embedding: {en.shape}\")\n",
    "print(f\"Batch shape after the end token embedding: {CustomLayer()(en).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFAp4Kzu9qtU"
   },
   "source": [
    "## 4. Build the encoder network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dSS6sC79qtU"
   },
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "def EncoderModel(input_shape):\n",
    "    input = tf.keras.layers.Input(shape=input_shape)\n",
    "    h = CustomLayer()(input)\n",
    "    h = tf.keras.layers.Masking(mask_value=0)(h)\n",
    "    output, hidden_state, cell_state = tf.keras.layers.LSTM(512, return_state=True)(h)\n",
    "    encoder_model = tf.keras.Model(input, [hidden_state, cell_state])\n",
    "    return encoder_model\n",
    "\n",
    "encoder_model = EncoderModel((13,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1hh2EXx9qtV",
    "outputId": "7a20b7e6-fcf9-4465-c697-60834bc6dd66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder model hidden state example shape: (16, 512)\n",
      "Encoder model cell state example shape: (16, 512)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Encoder model hidden state example shape: {encoder_model(en)[0].shape}\")\n",
    "print(f\"Encoder model cell state example shape: {encoder_model(en)[1].shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4iUVkmYk9qtW",
    "outputId": "1f4600cd-0404-48da-f635-7296baaf71bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 13, 128)]         0         \n",
      "_________________________________________________________________\n",
      "custom_layer_1 (CustomLayer) (None, 14, 128)           128       \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, 14, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 512), (None, 512) 1312768   \n",
      "=================================================================\n",
      "Total params: 1,312,896\n",
      "Trainable params: 1,312,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the encoder model summary\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFgMFWo19qtW"
   },
   "source": [
    "## 5. Build the decoder network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXXJT1Ai9qtX"
   },
   "outputs": [],
   "source": [
    "# Decoder model\n",
    "class DecoderModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DecoderModel, self).__init__(**kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, mask_zero=True)\n",
    "        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(len(tokenizer.word_index)+1)\n",
    "\n",
    "    def call(self, input, hidden_state=None, cell_state=None):\n",
    "        # hidden_state, cell_state from encoder\n",
    "        x = self.embedding(input)\n",
    "        x, hidden, cell = self.lstm(x, initial_state=[hidden_state, cell_state])\n",
    "        x = self.dense(x)\n",
    "        return x, hidden, cell\n",
    "\n",
    "decoder_model = DecoderModel()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24CKvb-Y9qtY",
    "outputId": "d977c952-dcb5-445b-b274-b31cb3aabd0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (16, 14, 5744)\n",
      "Hidden state shape: (16, 512)\n",
      "Cell state shape: (16, 512)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_hidden_state, encoder_cell_state = encoder_model(en)\n",
    "decoder_output, decoder_hidden_state, decoder_cell_state = decoder_model(de, encoder_hidden_state, encoder_cell_state)\n",
    "\n",
    "print(\"Output shape: {}\".format(decoder_output.shape))\n",
    "print(\"Hidden state shape: {}\".format(decoder_hidden_state.shape))\n",
    "print(\"Cell state shape: {}\\n\".format(decoder_cell_state.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzMnVZ339qtY",
    "outputId": "a1949c1b-77e1-477d-90dd-1eed5d269131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  735232    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  1312768   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  2946672   \n",
      "=================================================================\n",
      "Total params: 4,994,672\n",
      "Trainable params: 4,994,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icGC7D659qtZ"
   },
   "source": [
    "## 6. Make a custom training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0qyrx-D9qtZ"
   },
   "outputs": [],
   "source": [
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define a function that computes the forward and backward pass for the translation model\n",
    "@tf.function\n",
    "def loss_opt_fun(en_input, de_input, de_output, loss_f):\n",
    "    with tf.GradientTape() as g:\n",
    "        encoder_hidden_state, encoder_cell_state = encoder_model(en_input)\n",
    "        decoder_output, decoder_hidden_state, decoder_cell_state = decoder_model(de_input, encoder_hidden_state, encoder_cell_state)\n",
    "\n",
    "        loss = tf.math.reduce_mean(loss_f(de_output, decoder_output))\n",
    "        grads = g.gradient(loss, encoder_model.trainable_variables + decoder_model.trainable_variables)\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBkwuLrY9qta",
    "outputId": "ae22b716-e87d-4d3b-fd81-d3ae337d1ee2"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-05eca2f16c5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mde\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mde\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mde\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_opt_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m         \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2660\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_batch = []\n",
    "    for en, de in train_dataset:\n",
    "        decoder_inputs, decoder_outputs = de[:, :-1], de[:, 1:]\n",
    "        loss, grads = loss_opt_fun(en, decoder_inputs, decoder_outputs, loss_func)\n",
    "        optimizer.apply_gradients(zip(grads, encoder_model.trainable_variables + decoder_model.trainable_variables))\n",
    "        loss_batch += [loss]\n",
    "    \n",
    "    loss_train += [np.mean(loss_batch)]\n",
    "\n",
    "    loss_batch = []\n",
    "    for en, de in val_dataset:\n",
    "        (decoder_inputs, decoder_outputs) = de[:, :-1], de[:, 1:]\n",
    "        loss, grads = loss_opt_fun(en, decoder_inputs, decoder_outputs, loss_func)\n",
    "        loss_batch += [loss]\n",
    "\n",
    "    loss_val += [np.mean(loss_batch)]\n",
    "    print(f\"Epoch {epoch};\\tLoss train {loss_train[-1]};\\tLoss val {loss_val[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btTSr3ZQ9qta"
   },
   "outputs": [],
   "source": [
    "# Loss vs Epoch plots for the training and validation sets\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(loss_train)\n",
    "plt.plot(loss_val)\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Training', 'Validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE-HxA_A9qtc"
   },
   "source": [
    "## 7. Use the model to translate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ESYyPW59qtc"
   },
   "outputs": [],
   "source": [
    "random_ind = np.random.choice(20000,5)\n",
    "examples = []\n",
    "for ind in random_ind:\n",
    "    examples.append(data_examples[ind])\n",
    "english_sentences = [sentence.split('\\t')[0] for sentence in examples]\n",
    "processed_english = []\n",
    "for sentence in english_sentences:\n",
    "    processed_english.append(preprocess_sentence(sentence))\n",
    "    \n",
    "\n",
    "start = tokenizer.word_index['<start>']\n",
    "end = tokenizer.word_index['<end>']\n",
    "examples_tokens = []\n",
    "for p_english in processed_english:\n",
    "    english = tf.strings.split(p_english,sep = \" \")\n",
    "    english = embedding_layer(english)\n",
    "    english = tf.pad(english, [[13-len(english), 0], [0, 0]], constant_values = 0)\n",
    "    english = tf.expand_dims(english, 0)\n",
    "    hidden_state, cell_state = encoder_model(english)\n",
    "    translated_tokens = []\n",
    "    tf_token = tf.Variable([[start]])\n",
    "    while True:\n",
    "        output_1,hidden_state, cell_state = decoder_model(tf_token, hidden_state, cell_state)\n",
    "        output_2 = tf.argmax(output_1, 2).numpy()[0,0]\n",
    "        tf_token = tf.Variable([[output_2]])\n",
    "        if output_2 == end:\n",
    "            break\n",
    "        else:\n",
    "            translated_tokens.append(output_2)\n",
    "    examples_tokens.append(translated_tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nv_roAaP9qtd"
   },
   "outputs": [],
   "source": [
    "inv_german_index = {value:key for key,value in tokenizer.word_index.items()}\n",
    "german_sentences = []\n",
    "for example_token in examples_tokens:\n",
    "    output_words = []\n",
    "    for token in example_token:\n",
    "        output_words.append(inv_german_index[token])\n",
    "    output = \" \".join(output_words)\n",
    "    german_sentences.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BafQS0_t9qte"
   },
   "outputs": [],
   "source": [
    "table = PrettyTable(['English sentences', 'German Translations'])\n",
    "for english,german in zip(english_sentences,german_sentences):\n",
    "    table.add_row([english,german])\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
