{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6S2HVAkSt0p"
   },
   "source": [
    "#  CIFAR-10 Autoencoder\n",
    "\n",
    "Create a convolutional autoencoder for the [CIFAR10](https://www.tensorflow.org/datasets/catalog/cifar10) dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1mzy2J8_nc1"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPuX3RKxl8l5"
   },
   "outputs": [],
   "source": [
    "# Install packages for compatibility with the autograder\n",
    "!pip install tensorflow==2.6.0 --quiet\n",
    "!pip install keras==2.6.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EXwoz-KHtWO"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2Gs6Lyc_pd0"
   },
   "source": [
    "## Load and prepare the dataset\n",
    "\n",
    "The [CIFAR 10](https://www.tensorflow.org/datasets/catalog/cifar10) dataset already has train and test splits and you can use those in this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9F7YsCNIKSA"
   },
   "outputs": [],
   "source": [
    "# preprocessing function\n",
    "def map_image(image, label):\n",
    "  image = tf.cast(image, dtype=tf.float32)\n",
    "  image = image / 255.0\n",
    "\n",
    "  return image, image # dataset label is not used. replaced with the same image input.\n",
    "\n",
    "# parameters\n",
    "BATCH_SIZE = 128\n",
    "SHUFFLE_BUFFER_SIZE = 1024\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# use tfds.load() to fetch the 'train' split of CIFAR-10\n",
    "train_dataset = tfds.load('cifar10', split='train', as_supervised=True)\n",
    "\n",
    "# preprocess the dataset with the `map_image()` function above\n",
    "train_dataset = train_dataset.map(map_image)\n",
    "\n",
    "# shuffle and batch the dataset\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "# use tfds.load() to fetch the 'test' split of CIFAR-10\n",
    "test_dataset = tfds.load('cifar10', split='test', as_supervised=True)\n",
    "\n",
    "# preprocess the dataset with the `map_image()` function above\n",
    "test_dataset = test_dataset.map(map_image)\n",
    "\n",
    "# batch the dataset\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPyOgGJs_t98"
   },
   "source": [
    "## Build the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wr-Bok3lRgA3"
   },
   "outputs": [],
   "source": [
    "# suggested layers to use. feel free to add or remove as you see fit.\n",
    "from keras.layers import Conv2D, UpSampling2D\n",
    "\n",
    "# use the Sequential API (you can remove if you want to use the Functional API)\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "# use `model.add()` to add layers (if using the Sequential API)\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\", input_shape=(32, 32, 3))) # (16, 16, 64)\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\")) # (8, 8, 128)\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=\"relu\")) # (8, 8, 256)\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=\"relu\")) # (16, 16, 128)\n",
    "model.add(UpSampling2D(size=(2, 2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\", activation=\"relu\")) # (32, 32, 64)\n",
    "model.add(UpSampling2D(size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=3, kernel_size=(3, 3), padding=\"same\", activation=\"sigmoid\")) # (32, 32, 1)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', metrics=['accuracy'], loss='mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRWTAijKEVUC"
   },
   "source": [
    "## Configure training parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHIeD9eDETSk"
   },
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer='adam', metrics=['accuracy'], loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLQPhm1W_8dC"
   },
   "source": [
    "## Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMBimOnsRvg0"
   },
   "outputs": [],
   "source": [
    "# parameters \n",
    "train_steps = len(train_dataset) // BATCH_SIZE \n",
    "val_steps = len(test_dataset) // BATCH_SIZE\n",
    "\n",
    "\n",
    "model.fit(train_dataset, \n",
    "          steps_per_epoch=train_steps, \n",
    "          validation_data=test_dataset,\n",
    "          validation_steps=val_steps, \n",
    "          epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PT2l1c-SAaF4"
   },
   "source": [
    "## Model evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFncgqahSQhA"
   },
   "outputs": [],
   "source": [
    "result = model.evaluate(test_dataset, steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmpI4skkIA5L"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=12Fy-guiP-3tTPfc9IV2nHhqLvs7LwRo6\" width=\"75%\" height=\"75%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaRSkQPNAPT0"
   },
   "source": [
    "## Save the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLFpLP-c7rDR"
   },
   "outputs": [],
   "source": [
    "model.save('mymodel.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
