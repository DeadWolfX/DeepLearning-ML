{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, BatchNormalization, Conv2D, Dense, Flatten, Add\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fashion-MNIST overview image](data/fashion_mnist.png)\n",
    "\n",
    "#### The Fashion-MNIST dataset\n",
    "\n",
    "In this assignment, you will use the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist). It consists of a training set of 60,000 images of fashion items with corresponding labels, and a test set of 10,000 images. The images have been normalised and centred. The dataset is frequently used in machine learning research, especially as a drop-in replacement for the MNIST dataset. \n",
    "\n",
    "- H. Xiao, K. Rasul, and R. Vollgraf. \"Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms.\" arXiv:1708.07747, August 2017.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this programming assignment, we will take a smaller sample of the dataset to reduce the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the Fashion-MNIST dataset\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images.astype(np.float32)\n",
    "test_images = test_images.astype(np.float32)\n",
    "\n",
    "train_images = train_images[:5000] / 255.\n",
    "train_labels = train_labels[:5000]\n",
    "\n",
    "test_images = test_images / 255.\n",
    "\n",
    "train_images = train_images[..., np.newaxis]\n",
    "test_images = test_images[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset objects for the training and test sets\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_dataset = test_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset labels\n",
    "\n",
    "image_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create custom layers for the residual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ResidualBlock(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.batch_norm_1 = BatchNormalization(input_shape=input_shape)\n",
    "        self.conv_1 = Conv2D(input_shape[-1], (3,3), padding='same')\n",
    "        self.batch_norm_2 = BatchNormalization()\n",
    "        self.conv_2 = Conv2D(input_shape[-1], (3,3), padding='same')        \n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        x = self.batch_norm_1(inputs, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.batch_norm_2(inputs, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        return tf.add(inputs, x)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a model using your layer\n",
    "\n",
    "test_model = tf.keras.Sequential([ResidualBlock(input_shape=(28, 28, 1), name=\"residual_block\")])\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FiltersChangeResidualBlock(Layer):\n",
    "\n",
    "    def __init__(self, out_filters, **kwargs):\n",
    "        \n",
    "        super(FiltersChangeResidualBlock, self).__init__(**kwargs)\n",
    "        self.out_filters = out_filters       \n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.batch_norm_1 = BatchNormalization(input_shape=input_shape)\n",
    "        self.conv_1 = Conv2D(input_shape[-1], (3,3), padding='same')\n",
    "        self.batch_norm_2 = BatchNormalization()\n",
    "        self.conv_2 = Conv2D(self.out_filters, (3,3), padding='same')\n",
    "\n",
    "        self.conv_3 = Conv2D(self.out_filters, (1,1))         \n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        x = self.batch_norm_1(inputs, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.batch_norm_2(inputs, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        final = self.conv_3(inputs)\n",
    "        \n",
    "        return tf.add(x, final)     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your custom layer - the following should create a model using your layer\n",
    "\n",
    "test_model = tf.keras.Sequential([FiltersChangeResidualBlock(16, input_shape=(32, 32, 3), name=\"fc_resnet_block\")])\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a custom model that integrates the residual blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ResNetModel(Model):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        super(ResNetModel, self).__init__(**kwargs)\n",
    "\n",
    "        self.conv_1 = Conv2D(32, (7,7), strides=2)\n",
    "        self.resnet_1 = ResidualBlock()\n",
    "        self.conv_2 = Conv2D(32, (3,3), strides=2)\n",
    "        self.resnet_2 = FiltersChangeResidualBlock(64)\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = Dense(10, activation='softmax')        \n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.resnet_1(x, training)\n",
    "        x = self.conv_2(inputs)\n",
    "        x = self.resnet_2(x, training)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        return self.dense(x)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "resnet_model = ResNetModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimizer and loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Adam optimizer with a learning rate of 0.001, and the sparse categorical cross entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer and loss\n",
    "\n",
    "optimizer_obj = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the grad function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def grad(model, inputs, targets, loss):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = model(inputs)\n",
    "      loss_value = loss(targets, predictions)\n",
    "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        \n",
    "    return loss_value, grads    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_resnet(model, num_epochs, dataset, optimizer, loss, grad_fn):\n",
    "    \n",
    "    train_loss_results = []\n",
    "    train_accuracy_results = []\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "      epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "      epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "      for x, y in train_dataset:\n",
    "\n",
    "        loss_value, grads = grad_fn(model, x, y, loss)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        epoch_loss_avg(loss_value)\n",
    "        epoch_accuracy(to_categorical(y), model(x))\n",
    "\n",
    "      train_loss_results.append(epoch_loss_avg.result())\n",
    "      train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    return train_loss_results, train_accuracy_results    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 8 epochs\n",
    "\n",
    "train_loss_results, train_accuracy_results = train_resnet(resnet_model, 8, train_dataset, optimizer_obj, \n",
    "                                                          loss_obj, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(12, 5))\n",
    "\n",
    "axes[0].set_xlabel(\"Epochs\", fontsize=14)\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].set_title('Loss vs epochs')\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_title('Accuracy vs epochs')\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epochs\", fontsize=14)\n",
    "axes[1].plot(train_accuracy_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model performance on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the test loss and accuracy\n",
    "\n",
    "epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "for x, y in test_dataset:\n",
    "    model_output = resnet_model(x)\n",
    "    epoch_loss_avg(loss_obj(y, model_output))  \n",
    "    epoch_accuracy(to_categorical(y), model_output)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(epoch_loss_avg.result().numpy()))\n",
    "print(\"Test accuracy: {:.3%}\".format(epoch_accuracy.result().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model predictions on randomly selected test images\n",
    "\n",
    "num_test_images = test_images.shape[0]\n",
    "\n",
    "random_inx = np.random.choice(test_images.shape[0], 4)\n",
    "random_test_images = test_images[random_inx, ...]\n",
    "random_test_labels = test_labels[random_inx, ...]\n",
    "\n",
    "predictions = resnet_model(random_test_images)\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 12))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=-0.2)\n",
    "\n",
    "for i, (prediction, image, label) in enumerate(zip(predictions, random_test_images, random_test_labels)):\n",
    "    axes[i, 0].imshow(np.squeeze(image))\n",
    "    axes[i, 0].get_xaxis().set_visible(False)\n",
    "    axes[i, 0].get_yaxis().set_visible(False)\n",
    "    axes[i, 0].text(5., -2., f'Class {label} ({image_labels[label]})')\n",
    "    axes[i, 1].bar(np.arange(len(prediction)), prediction)\n",
    "    axes[i, 1].set_xticks(np.arange(len(prediction)))\n",
    "    axes[i, 1].set_xticklabels(image_labels, rotation=0)\n",
    "    pred_inx = np.argmax(prediction)\n",
    "    axes[i, 1].set_title(f\"Categorical distribution. Model prediction: {image_labels[pred_inx]}\")\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "tensor-flow-2-2",
   "graded_item_id": "2x3vn",
   "launcher_item_id": "QKXZc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
