{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AySnxYa6OCN"
      },
      "source": [
        "## Saving and loading models, with application to the EuroSat dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNGvo5fU6OCR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWEdjCAm6OCT"
      },
      "source": [
        "![EuroSAT overview image](data/eurosat_overview.jpg)\n",
        "\n",
        "#### The EuroSAT dataset\n",
        "\n",
        "In this assignment, you will use the [EuroSAT dataset](https://github.com/phelber/EuroSAT). It consists of 27000 labelled Sentinel-2 satellite images of different land uses: residential, industrial, highway, river, forest, pasture, herbaceous vegetation, annual crop, permanent crop and sea/lake. For a reference, see the following papers:\n",
        "- Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Patrick Helber, Benjamin Bischke, Andreas Dengel, Damian Borth. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.\n",
        "- Introducing EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification. Patrick Helber, Benjamin Bischke, Andreas Dengel. 2018 IEEE International Geoscience and Remote Sensing Symposium, 2018.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P42hwWxB6OCU"
      },
      "source": [
        "#### Import the data\n",
        "\n",
        "The dataset you will train your model on is a subset of the total data, with 4000 training images and 1000 testing images, with roughly equal numbers of each class. The code to import the data is provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUcu3roQ6OCW"
      },
      "outputs": [],
      "source": [
        "# Run this cell to import the Eurosat data\n",
        "\n",
        "def load_eurosat_data():\n",
        "    data_dir = 'data/'\n",
        "    x_train = np.load(os.path.join(data_dir, 'x_train.npy'))\n",
        "    y_train = np.load(os.path.join(data_dir, 'y_train.npy'))\n",
        "    x_test  = np.load(os.path.join(data_dir, 'x_test.npy'))\n",
        "    y_test  = np.load(os.path.join(data_dir, 'y_test.npy'))\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_eurosat_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNAwKv5s6OCX"
      },
      "source": [
        "#### Build the neural network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImPk2iBo6OCZ"
      },
      "outputs": [],
      "source": [
        "function name or arguments.\n",
        "\n",
        "def get_new_model(input_shape):\n",
        "  \n",
        "    model = Sequential([\n",
        "        Conv2D(filters=16, input_shape=input_shape, kernel_size=(3, 3), \n",
        "               activation='relu', padding='SAME', name='conv_1'),\n",
        "        Conv2D(filters=8, kernel_size=(3, 3), activation='relu', padding='SAME', name='conv_2'),\n",
        "        MaxPooling2D(pool_size=(8, 8), name='pool_1'),\n",
        "        Flatten(name='flatten'),\n",
        "        Dense(units=32, activation='relu', name='dense_1'),\n",
        "        Dense(units=10, activation='softmax', name='dense_2')\n",
        "    ])\n",
        "    \n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IikDCEbb6OCb"
      },
      "source": [
        "#### Compile and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O54hG0_6OCc"
      },
      "outputs": [],
      "source": [
        "# Run your function to create the model\n",
        "\n",
        "model = get_new_model(x_train[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppe1dLyV6OCe"
      },
      "outputs": [],
      "source": [
        "# Run this cell to define a function to evaluate a model's test accuracy\n",
        "\n",
        "def get_test_accuracy(model, x_test, y_test):\n",
        "    \"\"\"Test model classification accuracy\"\"\"\n",
        "    test_loss, test_acc = model.evaluate(x=x_test, y=y_test, verbose=0)\n",
        "    print('accuracy: {acc:0.3f}'.format(acc=test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqDpUkJd6OCf",
        "outputId": "ecf9ca25-4aac-4d18-d51f-d2a42c4b98fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_1 (Conv2D)              (None, 64, 64, 16)        448       \n",
            "_________________________________________________________________\n",
            "conv_2 (Conv2D)              (None, 64, 64, 8)         1160      \n",
            "_________________________________________________________________\n",
            "pool_1 (MaxPooling2D)        (None, 8, 8, 8)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                16416     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 18,354\n",
            "Trainable params: 18,354\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "accuracy: 0.090\n"
          ]
        }
      ],
      "source": [
        "# Print the model summary and calculate its initialised test accuracy\n",
        "\n",
        "model.summary()\n",
        "get_test_accuracy(model, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrJWlg1v6OCf"
      },
      "source": [
        "#### Create checkpoints to save model during training, with a criterion\n",
        "\n",
        "- `checkpoint_every_epoch`: checkpoint that saves the model weights every epoch during training\n",
        "- `checkpoint_best_only`: checkpoint that saves only the weights with the highest validation accuracy. Use the testing data as the validation data.\n",
        "- `early_stopping`: early stopping object that ends training if the validation accuracy has not improved in 3 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFD-Hr-_6OCg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_checkpoint_every_epoch():\n",
        " \n",
        "    return ModelCheckpoint(\n",
        "        filepath = 'checkpoints_every_epoch/checkpoint_{epoch:03d}',\n",
        "        frequency='epoch',\n",
        "        save_weights_only=True, \n",
        "        verbose=1)\n",
        "    \n",
        "\n",
        "\n",
        "def get_checkpoint_best_only():\n",
        "\n",
        "    return ModelCheckpoint(\n",
        "        filepath = 'checkpoints_best_only/checkpoint',\n",
        "        frequency='epoch',\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        save_weights_only=True, \n",
        "        verbose=1)    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AmNxSnK6OCh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_early_stopping():\n",
        "\n",
        "    return EarlyStopping(patience=3, monitor='val_accuracy')  \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcAQkbjD6OCh"
      },
      "outputs": [],
      "source": [
        "# Run this cell to create the callbacks\n",
        "\n",
        "checkpoint_every_epoch = get_checkpoint_every_epoch()\n",
        "checkpoint_best_only = get_checkpoint_best_only()\n",
        "early_stopping = get_early_stopping()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGpgoNSd6OCi"
      },
      "source": [
        "#### Train model using the callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZaSHRdn6OCj",
        "outputId": "698a17d4-a62a-4b7b-b13d-027f4c3ef064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 4000 samples, validate on 1000 samples\n",
            "Epoch 1/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 1.9462 - accuracy: 0.2704\n",
            "Epoch 00001: saving model to checkpoints_every_epoch/checkpoint_001\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.38100, saving model to checkpoints_best_only/checkpoint\n",
            "4000/4000 [==============================] - 87s 22ms/sample - loss: 1.9428 - accuracy: 0.2713 - val_loss: 1.6531 - val_accuracy: 0.3810\n",
            "Epoch 2/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 1.4744 - accuracy: 0.4594\n",
            "Epoch 00002: saving model to checkpoints_every_epoch/checkpoint_002\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.38100 to 0.43100, saving model to checkpoints_best_only/checkpoint\n",
            "4000/4000 [==============================] - 87s 22ms/sample - loss: 1.4750 - accuracy: 0.4597 - val_loss: 1.4492 - val_accuracy: 0.4310\n",
            "Epoch 3/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 1.3203 - accuracy: 0.5146\n",
            "Epoch 00003: saving model to checkpoints_every_epoch/checkpoint_003\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.43100 to 0.51700, saving model to checkpoints_best_only/checkpoint\n",
            "4000/4000 [==============================] - 86s 22ms/sample - loss: 1.3206 - accuracy: 0.5145 - val_loss: 1.2761 - val_accuracy: 0.5170\n",
            "Epoch 4/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 1.2104 - accuracy: 0.5504\n",
            "Epoch 00004: saving model to checkpoints_every_epoch/checkpoint_004\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.51700 to 0.51900, saving model to checkpoints_best_only/checkpoint\n",
            "4000/4000 [==============================] - 87s 22ms/sample - loss: 1.2078 - accuracy: 0.5510 - val_loss: 1.2952 - val_accuracy: 0.5190\n",
            "Epoch 5/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 1.1484 - accuracy: 0.5799\n",
            "Epoch 00005: saving model to checkpoints_every_epoch/checkpoint_005\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.51900 to 0.54500, saving model to checkpoints_best_only/checkpoint\n",
            "4000/4000 [==============================] - 91s 23ms/sample - loss: 1.1450 - accuracy: 0.5815 - val_loss: 1.2245 - val_accuracy: 0.5450\n",
            "Epoch 6/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 1.1167 - accuracy: 0.5940\n",
            "Epoch 00006: saving model to checkpoints_every_epoch/checkpoint_006\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.54500 to 0.56800, saving model to checkpoints_best_only/checkpoint\n",
            "4000/4000 [==============================] - 85s 21ms/sample - loss: 1.1153 - accuracy: 0.5943 - val_loss: 1.1415 - val_accuracy: 0.5680\n",
            "Epoch 7/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 1.0246 - accuracy: 0.6268\n",
            "Epoch 00007: saving model to checkpoints_every_epoch/checkpoint_007\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.56800\n",
            "4000/4000 [==============================] - 88s 22ms/sample - loss: 1.0254 - accuracy: 0.6260 - val_loss: 1.1471 - val_accuracy: 0.5680\n",
            "Epoch 8/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 0.9937 - accuracy: 0.6424\n",
            "Epoch 00008: saving model to checkpoints_every_epoch/checkpoint_008\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.56800 to 0.60400, saving model to checkpoints_best_only/checkpoint\n",
            "4000/4000 [==============================] - 85s 21ms/sample - loss: 0.9931 - accuracy: 0.6425 - val_loss: 1.0727 - val_accuracy: 0.6040\n",
            "Epoch 9/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 0.9579 - accuracy: 0.6527\n",
            "Epoch 00009: saving model to checkpoints_every_epoch/checkpoint_009\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.60400 to 0.63800, saving model to checkpoints_best_only/checkpoint\n",
            "4000/4000 [==============================] - 85s 21ms/sample - loss: 0.9570 - accuracy: 0.6528 - val_loss: 1.0407 - val_accuracy: 0.6380\n",
            "Epoch 10/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 0.9155 - accuracy: 0.6719\n",
            "Epoch 00010: saving model to checkpoints_every_epoch/checkpoint_010\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.63800\n",
            "4000/4000 [==============================] - 86s 22ms/sample - loss: 0.9184 - accuracy: 0.6712 - val_loss: 1.0489 - val_accuracy: 0.6340\n",
            "Epoch 11/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 0.9137 - accuracy: 0.6651\n",
            "Epoch 00011: saving model to checkpoints_every_epoch/checkpoint_011\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.63800\n",
            "4000/4000 [==============================] - 86s 21ms/sample - loss: 0.9126 - accuracy: 0.6650 - val_loss: 1.0043 - val_accuracy: 0.6300\n",
            "Epoch 12/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 0.8480 - accuracy: 0.6875\n",
            "Epoch 00012: saving model to checkpoints_every_epoch/checkpoint_012\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.63800 to 0.65900, saving model to checkpoints_best_only/checkpoint\n",
            "4000/4000 [==============================] - 86s 22ms/sample - loss: 0.8506 - accuracy: 0.6862 - val_loss: 0.9412 - val_accuracy: 0.6590\n",
            "Epoch 13/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 0.8468 - accuracy: 0.6966\n",
            "Epoch 00013: saving model to checkpoints_every_epoch/checkpoint_013\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.65900\n",
            "4000/4000 [==============================] - 87s 22ms/sample - loss: 0.8488 - accuracy: 0.6952 - val_loss: 1.0800 - val_accuracy: 0.5990\n",
            "Epoch 14/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 0.8225 - accuracy: 0.7021\n",
            "Epoch 00014: saving model to checkpoints_every_epoch/checkpoint_014\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.65900\n",
            "4000/4000 [==============================] - 85s 21ms/sample - loss: 0.8234 - accuracy: 0.7013 - val_loss: 0.9728 - val_accuracy: 0.6350\n",
            "Epoch 15/50\n",
            "3968/4000 [============================>.] - ETA: 0s - loss: 0.7972 - accuracy: 0.7117\n",
            "Epoch 00015: saving model to checkpoints_every_epoch/checkpoint_015\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.65900\n",
            "4000/4000 [==============================] - 85s 21ms/sample - loss: 0.7978 - accuracy: 0.7110 - val_loss: 0.9516 - val_accuracy: 0.6460\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f730837a898>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train model using the callbacks you just created\n",
        "\n",
        "callbacks = [checkpoint_every_epoch, checkpoint_best_only, early_stopping]\n",
        "model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49_j11lk6OCj"
      },
      "source": [
        "#### Create new instance of model and load on both sets of weights\n",
        "\n",
        "Now you will use the weights you just saved in a fresh model. You should create two functions, both of which take a freshly instantiated model instance:\n",
        "- `model_last_epoch` should contain the weights from the latest saved epoch\n",
        "- `model_best_epoch` should contain the weights from the saved epoch with the highest testing accuracy\n",
        "\n",
        "_Hint: use the_ `tf.train.latest_checkpoint` _function to get the filename of the latest saved checkpoint file. Check the docs_ [_here_](https://www.tensorflow.org/api_docs/python/tf/train/latest_checkpoint)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNaCwIJQ6OCk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_model_last_epoch(model):\n",
        "  \n",
        "    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir='checkpoints_every_epoch'))\n",
        "    return model    \n",
        "    \n",
        "    \n",
        "    \n",
        "def get_model_best_epoch(model):\n",
        "   \n",
        "    model.load_weights('checkpoints_best_only/checkpoint')\n",
        "    return model    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjLm-c3-6OCk",
        "outputId": "12d34af4-1eb2-4072-ae3c-acba9d42ba5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model with last epoch weights:\n",
            "accuracy: 0.646\n",
            "\n",
            "Model with best epoch weights:\n",
            "accuracy: 0.659\n"
          ]
        }
      ],
      "source": [
        "# create two models: one with the weights from the last training\n",
        "# epoch, and one with the weights leading to the highest validation (testing) accuracy.\n",
        "# Verify that the second has a higher validation (testing) accuarcy.\n",
        "\n",
        "model_last_epoch = get_model_last_epoch(get_new_model(x_train[0].shape))\n",
        "model_best_epoch = get_model_best_epoch(get_new_model(x_train[0].shape))\n",
        "print('Model with last epoch weights:')\n",
        "get_test_accuracy(model_last_epoch, x_test, y_test)\n",
        "print('')\n",
        "print('Model with best epoch weights:')\n",
        "get_test_accuracy(model_best_epoch, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5JKaFlR6OCl"
      },
      "source": [
        "#### Load, from scratch, a model trained on the EuroSat dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reVJCeFH6OCl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_model_eurosatnet():\n",
        "    \n",
        "    model= load_model('models/EuroSatNet.h5')\n",
        "    return model    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-wSpDJh6OCm",
        "outputId": "15f2639c-0e73-460a-d4ee-41b19c33bf0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_1 (Conv2D)              (None, 64, 64, 16)        448       \n",
            "_________________________________________________________________\n",
            "conv_2 (Conv2D)              (None, 64, 64, 16)        6416      \n",
            "_________________________________________________________________\n",
            "pool_1 (MaxPooling2D)        (None, 32, 32, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv_3 (Conv2D)              (None, 32, 32, 16)        2320      \n",
            "_________________________________________________________________\n",
            "conv_4 (Conv2D)              (None, 32, 32, 16)        6416      \n",
            "_________________________________________________________________\n",
            "pool_2 (MaxPooling2D)        (None, 16, 16, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv_5 (Conv2D)              (None, 16, 16, 16)        2320      \n",
            "_________________________________________________________________\n",
            "conv_6 (Conv2D)              (None, 16, 16, 16)        6416      \n",
            "_________________________________________________________________\n",
            "pool_3 (MaxPooling2D)        (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv_7 (Conv2D)              (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "conv_8 (Conv2D)              (None, 8, 8, 16)          6416      \n",
            "_________________________________________________________________\n",
            "pool_4 (MaxPooling2D)        (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                8224      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 41,626\n",
            "Trainable params: 41,626\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "accuracy: 0.810\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to print a summary of the EuroSatNet model, along with its validation accuracy.\n",
        "\n",
        "model_eurosatnet = get_model_eurosatnet()\n",
        "model_eurosatnet.summary()\n",
        "get_test_accuracy(model_eurosatnet, x_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "tensor-flow-2-1",
      "graded_item_id": "JaRY0",
      "launcher_item_id": "mJ8fg"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}